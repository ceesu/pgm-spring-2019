---
layout: distill
title: "Lecture 02: Bayesian Networks"
description: TBD
date: 2019-01-16

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Cathy Su
    url: "https://ceesu.github.io"   
  - name: Angel C. Hernandez
    url: "https://github.com/ahernandez105"
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

***
# Readings
## The Bayesian network representation <d-cite key="Koller:2009:PGM:1795555"></d-cite>

### 3.1.1  Exploiting independence Properties
#### Standard vs compact parametrization of  independent random variables
Given random variables $$X_i$$ each representing the outcome of an independent coin toss, the standard parametrization of their joint distribution would be as follows:

$$ P(X_1, ... X_n) = P(X_1 = x_1, X_2=x_2, ..., X_n = x_n) = P(X_1 = x_1)P(X_2=x_2)...P(X_n=x_n) $$

Note there are 2 possibilities for each outcome $$x_i$$, this representation requires $$2^n$$ total number of parameters. 

One simple way to reduce the number of parameters needed, would be to represent the probability that each coin toss lands heads as $$ \theta_1, ... \theta_n $$.  Then we have the following compact representation requiring only $$n$$ parameters:

$$ P(X_1, ... X_n) = \prod_i \theta_{X_i} $$

### 3.1.3 Naive Bayes
We can further express the joint distribution in terms of conditional probabilities. This is done in the Naive Bayes model.

 Instances of this model have will include:
* class : some category $$C \in \{c^1, ...c^k\}$$ which gives a prior on the value of each feature
* features: observed properties $$X_1, ... X_k$$ 

The model makes the strong  'naive' conditional independence assumption:

$$ P(x_i | C,  x_1, \dots, x_n) = P(x_i | C) $$

In words, _features are conditionally independent, given the class of the instance of this model_.  Thus the joint distribution of the Naive Bayes model factorizes as follows:

$$ P(C, X_1, ... X_n) = P(C) \prod_i P(X_i |C)$$

### 3.2.1 Bayesian networks
Unlike in the Naive Bayes model, Bayesian networks can also represent distributions that do not satisfy the naive conditional independence assumption. 

The Bayesian network is represented by a directed acyclic graph $$\mathcal{G}$$  whose nodes represent random variables $$X_1, ...X_n $$., which serves to represent the joint distribution compactly, by representing a set of conditional probability assumptions about the distribution. 

Specifically (Definition )

### 3.2.3 Graphs and Distributions

This section includes  $$P$$ satisfies the local independencies associated with  $$\mathcal{G} \iff P$$ is representable as a set of conditional probability distributions (CPDs) associated with $$\mathcal{G}$$. 

#### I-Map

### Box 3.C Knowledge engineering

 Building a Bayesian Network in the real world requires many steps including:

* _picking variables precisely_: we need some variables that can be observed, and their domain should be specified. Sometimes introducing hidden variables will help to render the observed variables conditionally independent.
* _picking a structure consistent with the causal order_: choose enough variables to approximate the causal relationships. 
* _picking probability estimates for events in the network_: data collection may be difficult but small errors have little effect (though the network is sensitive to events assigned a zero probability, and to relative size of probabilities of events)

Done correctly, the model will be accurate as well as not too complex to use (see Box 3.D).

### 3.3.1 D-separation

Objective: determine which independencies hold for every distribution $$P$$ which factorizes over $$G$$.

By examining 2-edge connections between nodes $$X$$, $$Y$$ and $$Z$$

Further, "directed separation" (d-separation) 

#### Definition 3.6 active trail
Let $$\mathcal{G}$$ be a BN structure and. $$ X_1 <=> , ... <=> X_n  $$ be a trail in $$\mathcal{G}$$. Let $$Z$$
Be a subset of observed variables. Then the trail $$ X_1 <=> , ... <=> X_n  $$ is active given $$Z  $$ if:

* Whenever we have.a v-structure $$ X_{I-1}  \to X_i \leftarrow X_{I+1}  $$ then $$X_i$$ or one of its descendants are in $$Z$$
* No other node along the trail is in $$Z$$

#### Definition 3.7 D-separation 


### 3.3.2 Soundness and Completeness

*Soundness:* if $$ X, Y $$ are d-separated given $$ Z, \implies X \perp Y | Z $$.  i.e. independence determined by d-separation is satisfied by the underlying distribution. 

*Completeness:*  If two variables $$ X \perp Y | Z $$ then they are d-separated. 

For proof, please see the textbook.

***

# In class notes

Motivation: representing joint distributions over some random variables is computationally expensive, so we need methodologies to represent joint distributions compactly.

## Two types of Graphical Models <d-cite key="Blei:2015:BasicsPGM"></d-cite><d-cite key="Jordan:2003:PGM"></d-cite>
### Directed Graphs (Bayesian Networks)
An acyclic graph, $$\mathcal{G}$$, is made up of a set of nodes, $$\mathcal{V}$$, and a set 
of directed edges, $$\mathcal{E}$$, where edges represent a causality relationship between nodes. Nodes in the graph represent a set of random variables, $$\{x_1,...,x_N\}$$, where there is a one-to-one map between nodes and random variables. 

<img src="{{ '/assets/img/notes/lecture-02/directed-graph.png' | relative_url }}" height ="270" width = "480"/>

We define  $$x_{\pi_i}$$ and $$x_{\iota_i}$$ as the set of parent nodes and non-ancestor nodes 
for the $$i$$ node, respectively. The topology of a directed graph asserts the set of conditional independence statements:
$$ \{ x_i \, \bot \, x_{\iota_i} \, | \, x_{\pi_i} \}$$
As a result, the joint probability of the above directed graph can be written as 
followed:

$p(\mathbf{x}) = \prod\limits_{i =1}^N p(x_i \, | \, x_{\pi_i})$
$p(\textbf{x}) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(x_6|x_2,x_5)$

A directed graph provides the skeleton for representing a joint distribution compactly
in a factorized way. 
### Undirected Graphs (Markov Random Fields) <d-cite key="Jordan:2003:PGM"></d-cite>
An undirected graph contains nodes that are connected via non-directional edges.
<img src="{{ '/assets/img/notes/lecture-02/undirected-graph.png' | relative_url }}" height ="270" width = "480" />
The joint probability of the above undirected graph can be written as followed:

$$p(\textbf{x}) = \frac{1}{Z}exp\bigg(E(x_1)+ E(x_2,x_1)+E(x_3,x_1)+E(x_4,x_2) + E(x_5,x_2,x_6)\bigg)$$

## The Dishonest Casino
Let $$\mathcal{D}$$  be the set $$\{(x_1,y_1),..,(x_N,y_N)\}$$ where is $$x_t$$  is the outcome of a casino's
die roll, $$x\,\epsilon \, (1,2,3,4,5,6)$, and $$y_t$$ is outcome of whether the die was fair or bias,$$y \, \epsilon \, (0,1)$$. 
The bias die follows the following distributions:
p(x=1)|p(x=2)|p(x=3)|p(x=4)|p(x=5)|p(x=6)|
-----|-------|------|------|------|------|
0.01|0.01|0.01|0.01|0.01|0.5|

Some questions we might want to know using $\mathcal{D}$ are:

* **Evaluation:** How likely is $$\mathcal{D}$$, given our model of casino?
* **Decoding**: What portion of the sequence was generated
with the fair die, and what portion with the loaded die?
* **Learning:** How "loaded" is the loaded die? How "fair" is the fair die? How often does the casino player change from fair die to loaded die, and back?

One way we could model this casino problem is as a Hidden Markov Model where $$x_t$$ are our
observed variables and $$y_t$$ are our hidden variables.
<img src="{{ '/assets/img/notes/lecture-02/hmm.png' | relative_url }}" height ="270" width = "480"/>
The hidden variables all share the _Markov property_ that the the past is conditionally 
independent of the future given the present:

$$y_{t-1} \, \bot \, \{y_{t+1},...,y_{N}\} \, | \, y_t$$

This property is also explicitly highlighted in the topology of the graph. Furthermore, 
the joint probability mass function can be written as followed:
$$p(\textbf{x,y}) = p(y_1)\prod\limits_{t=1}^N p(x_t\,|\,y_t)\prod\limits_{t=2}^{N} p(y_t\,|\,y_{t-1})$$
The marginal and posterior distributions can be computed as followed:
$$Marginal: p(\textbf{x}) = \sum\limits_{y_{1}}\cdots\sum\limits_{y_N} p(\textbf{x,y})$$

$$Posterior: p(\textbf{y}\,|\,\textbf{x}) = \frac{p(\textbf{x,y})}{p(\textbf{x})}$$

Test image

<img src="{{ '/assets/img/notes/lecture-02/cmu-logo.png' | relative_url }}"  />


